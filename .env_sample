# Make sure you dont add single or double quotes in the variable values, example : 
# correct format is : OPENAI_BASE_URL=http://localhost:1234/v1
# but incorrect format is : OPENAI_BASE_URL="http://localhost:1234/v1"

CHARACTER_IDENTIFICATION_LLM_BASE_URL=<Needed for character identification flow. Use atleast 20,000 context window size. Set your model provider base url ex. for openai it is https://api.openai.com/v1/ or for LM Studio it is http://localhost:1234/v1>
CHARACTER_IDENTIFICATION_LLM_API_KEY=<your model provider api key, ex. for lm-studio lm-studio>
CHARACTER_IDENTIFICATION_LLM_MODEL_NAME=<your model name ex. Qwen/Qwen3-30B-A3B-Instruct-2507. Use atleast 20,000 context window size.>
EMOTION_TAG_ADDITION_LLM_BASE_URL=<Needed for emotion tag addition flow. Use atleast 8192 context window size. Set your model provider base url ex. for openai it is https://api.openai.com/v1/ or for LM Studio it is http://localhost:1234/v1>
EMOTION_TAG_ADDITION_LLM_API_KEY=<your model provider api key, ex. for lm-studio lm-studio>
EMOTION_TAG_ADDITION_LLM_MODEL_NAME=<your model name ex. openai/gpt-oss-20b. Use atleast 8192 context window size.>
EMOTION_TAG_ADDITION_LLM_MAX_PARALLEL_REQUESTS_BATCH_SIZE=<The max number of parallel requests to make while calling the LLM for tag addition. This step is used in the emotion adding flow. Make sure your inference provider supports parallel inference for example: llama-cpp with --parallel flag. You can set its value to greater than or equal to 1>
TTS_BASE_URL=<Audio generator fastapi exposed openai compatible base url ex. http://localhost:8880/v1 for both Kokoro and Orpheus TTS FastAPI>
TTS_API_KEY=<specify if needed ex. not-needed or dummy-key>
TTS_MODEL=<TTS model to use. Choose either kokoro or orpheus>
NO_THINK_MODE=<whether to disable thinking mode in LLMs like Qwen3, R1 etc for faster inference. Takes in values of either true or false. Default is true>
TTS_MAX_PARALLEL_REQUESTS_BATCH_SIZE=<Choose the value based on this guide: https://github.com/prakharsr/audiobook-creator/?tab=readme-ov-file#parallel-batch-inferencing-of-audio-for-faster-audio-generation.